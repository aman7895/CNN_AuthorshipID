{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "authors=[123,80, 75]\n",
    "doc = 204\n",
    "\n",
    "def getWordAuthData(PORT, authors, doc, documentTable = 'aman_content', chunk_size = 1000):\n",
    "    df = pd.DataFrame()\n",
    "    conn = None\n",
    "    output = []\n",
    "    i = 1\n",
    "    # nltk.download('punkt')\n",
    "    try:\n",
    "        conn = psycopg2.connect(user=\"stylometry\", password=\"stylometry\",\n",
    "                                database=\"stylometry\", host=\"localhost\", port=PORT)\n",
    "        cur = conn.cursor()\n",
    "        query = \"SELECT author_id, doc_content FROM \" + str(documentTable) + \" WHERE author_id IN (\"\n",
    "        flag = False\n",
    "        for auth in authors:\n",
    "            if not flag:\n",
    "                query = query + str(auth)\n",
    "                flag = True\n",
    "            else:\n",
    "                query = query + \", \" + str(auth)\n",
    "        query = query + \") AND doc_id <> '\" + str(doc) + \"' ;\"\n",
    "        cur.execute(query)\n",
    "        print(\"Execution completed\")\n",
    "        rows = cur.fetchall()\n",
    "        print(\"Read completed\")\n",
    "        print(\"Number of rows: %s\" % (len(rows)))\n",
    "        for row in rows:\n",
    "            tokens = nltk.word_tokenize(row[1].decode(\"utf8\"))\n",
    "            chunk1 = []\n",
    "            for x in tokens:\n",
    "                if (i < chunk_size):\n",
    "                    chunk1.append(x.encode(\"utf8\"))\n",
    "                    i += 1\n",
    "                else:\n",
    "                    chunk1.append(x.encode(\"utf8\"))\n",
    "                    xx = ' '.join(chunk1)\n",
    "                    xx = str(xx)\n",
    "                    chunk1 = []\n",
    "                    output.append([row[0], xx])\n",
    "                    i = 1\n",
    "            if len(chunk1) > 0:\n",
    "                xx = ' '.join(chunk1)\n",
    "                xx = str(xx)\n",
    "                chunk1 = []\n",
    "                output.append([row[0], xx])\n",
    "                i = 1\n",
    "\n",
    "        df = pd.DataFrame(output, columns=[\"author_id\", \"doc_content\"])\n",
    "        print(df.dtypes)\n",
    "        print(\"Data Frame created: Shape: %s\" % (str(df.shape)))\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        print('Error %s' % e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    \n",
    "    print df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TRIAL\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "authors=[123,80, 75]\n",
    "doc = 204\n",
    "\n",
    "def getWordAuthData(PORT, authors, doc, documentTable = 'aman_content', chunk_size = 1000):\n",
    "    df = pd.DataFrame()\n",
    "    conn = None\n",
    "    output = []\n",
    "    i = 1\n",
    "    # nltk.download('punkt')\n",
    "    try:\n",
    "        \n",
    "        with SSHTunnelForwarder((\"srn01.cs.cityu.edu.hk\", 22),\n",
    "                            ssh_username='stylometry',\n",
    "                            ssh_password='stylometry',\n",
    "                            remote_bind_address=('localhost', 5432),\n",
    "                            local_bind_address=('localhost', 5400)):\n",
    "            \n",
    "            conn = psycopg2.connect(user=\"stylometry\", password=\"stylometry\",\n",
    "                                database=\"stylometry\", host=\"localhost\", port=5400)\n",
    "            cur = conn.cursor()\n",
    "            query = \"SELECT * from aman_content;\"\n",
    "            cur.execute(query)\n",
    "            print(\"Execution completed\")\n",
    "            rows = cur.fetchall() \n",
    "            print(\"Read completed\")\n",
    "            print(\"Number of rows: %s\" % (len(rows)))\n",
    "            \n",
    "            \n",
    "            cur = conn.cursor()\n",
    "            query = \"SELECT author_id, doc_content FROM \" + str(documentTable) + \" WHERE author_id IN (\"\n",
    "            flag = False\n",
    "            for auth in authors:\n",
    "                if not flag:\n",
    "                    query = query + str(auth)\n",
    "                    flag = True\n",
    "                else:\n",
    "                    query = query + \", \" + str(auth)\n",
    "            query = query + \") AND doc_id <> '\" + str(doc) + \"' ;\"\n",
    "            cur.execute(query)\n",
    "            print(\"Execution completed\")\n",
    "            rows = cur.fetchall()\n",
    "            print(\"Read completed\")\n",
    "            print(\"Number of rows: %s\" % (len(rows)))\n",
    "            for row in rows:\n",
    "                tokens = nltk.word_tokenize(row[1].decode(\"utf8\"))\n",
    "                chunk1 = []\n",
    "                for x in tokens:\n",
    "                    if (i < chunk_size):\n",
    "                        chunk1.append(x.encode(\"utf8\"))\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        chunk1.append(x.encode(\"utf8\"))\n",
    "                        xx = ' '.join(chunk1)\n",
    "                        xx = str(xx)\n",
    "                        chunk1 = []\n",
    "                        output.append([row[0], xx])\n",
    "                        i = 1\n",
    "                if len(chunk1) > 0:\n",
    "                    xx = ' '.join(chunk1)\n",
    "                    xx = str(xx)\n",
    "                    chunk1 = []\n",
    "                    output.append([row[0], xx])\n",
    "                    i = 1\n",
    "\n",
    "            df = pd.DataFrame(output, columns=[\"author_id\", \"doc_content\"])\n",
    "            print(df.dtypes)\n",
    "            print(\"Data Frame created: Shape: %s\" % (str(df.shape)))\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        print('Error %s' % e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    print df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-e8ae338221f3>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-e8ae338221f3>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(\"Execution completed\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "cur.execute(query)\n",
    "        print(\"Execution completed\")\n",
    "        rows = cur.fetchall()\n",
    "        print(\"Read completed\")\n",
    "        print(\"Number of rows: %s\" % (len(rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-c30ada2f137a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-c30ada2f137a>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    cur = conn.cursor()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(user=\"stylometry\", password=\"stylometry\",\n",
    "                                database=\"stylometry_v2\", host=\"localhost\", port=PORT)\n",
    "        cur = conn.cursor()\n",
    "        query = \"SELECT author_id, doc_content FROM \" + str(documentTable) + \" WHERE author_id IN (\"\n",
    "        flag = False\n",
    "        for auth in authors:\n",
    "            if not flag:\n",
    "                query = query + str(auth)\n",
    "                flag = True\n",
    "            else:\n",
    "                query = query + \", \" + str(auth)\n",
    "        query = query + \") AND doc_id <> '\" + str(doc) + \"' ;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution completed\n",
      "Read completed\n",
      "Number of rows: 3773\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "with SSHTunnelForwarder((\"srn01.cs.cityu.edu.hk\", 22),\n",
    "                            ssh_username='stylometry',\n",
    "                            ssh_password='stylometry',\n",
    "                            remote_bind_address=('localhost', 5432),\n",
    "                            local_bind_address=('localhost', 5400)):\n",
    "    conn = psycopg2.connect(user=\"stylometry\", password=\"stylometry\",\n",
    "                                database=\"stylometry\", host=\"localhost\", port=5400)\n",
    "    cur = conn.cursor()\n",
    "    query = \"SELECT * from aman_content;\"\n",
    "    cur.execute(query)\n",
    "    print(\"Execution completed\")\n",
    "    rows = cur.fetchall() \n",
    "    print(\"Read completed\")\n",
    "    print(\"Number of rows: %s\" % (len(rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.preprocessing import one_hot\n",
    "\n",
    "y = [0, 1, 2, 1, 2]\n",
    "one_hot(y, num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 1, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train[0] - 1\n",
    "y_train = one_hot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re #This is to parse the HTML code from the given text\n",
    "from sshtunnel import SSHTunnelForwarder #This is to connect to the local Database\n",
    "\n",
    "authors=[123,80, 75]\n",
    "doc = 204\n",
    "\n",
    "\n",
    "def getWordAuthData(authors, doc, documentTable = 'aman_content', chunk_size = 1000):\n",
    "    df = pd.DataFrame()\n",
    "    conn = None\n",
    "    output = []\n",
    "    i = 1\n",
    "    # nltk.download('punkt')\n",
    "    try:\n",
    "        \n",
    "        with SSHTunnelForwarder((\"srn01.cs.cityu.edu.hk\", 22),\n",
    "                            ssh_username='stylometry',\n",
    "                            ssh_password='stylometry',\n",
    "                            remote_bind_address=('localhost', 5432),\n",
    "                            local_bind_address=('localhost', 5400)):\n",
    "            \n",
    "            conn = psycopg2.connect(user=\"stylometry\", password=\"stylometry\",\n",
    "                                database=\"stylometry\", host=\"localhost\", port=5400)\n",
    "            \n",
    "            \n",
    "            cur = conn.cursor()\n",
    "            query = \"SELECT author_id, doc_content FROM \" + str(documentTable) + \" WHERE author_id IN (\"\n",
    "            flag = False\n",
    "            for auth in authors:\n",
    "                if not flag:\n",
    "                    query = query + str(auth)\n",
    "                    flag = True\n",
    "                else:\n",
    "                    query = query + \", \" + str(auth)\n",
    "            query = query + \") AND doc_id <> '\" + str(doc) + \"' ;\"\n",
    "            cur.execute(query)\n",
    "            print(\"Execution completed\")\n",
    "            rows = cur.fetchall()\n",
    "            \n",
    "            print(\"Read completed\")\n",
    "            print(\"Number of rows: %s\" % (len(rows)))\n",
    "            for row in rows:\n",
    "                #tokens = nltk.word_tokenize(row[1])\n",
    "               \n",
    "                temp = re.sub('<[^<]+?>', '', row[1])\n",
    "                chars = list(temp)\n",
    "                \n",
    "                chunk1 = []\n",
    "                for x in chars:\n",
    "                    if (i < chunk_size):\n",
    "                        chunk1.append(x)\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        chunk1.append(x)\n",
    "                        xx = ''.join(chunk1)\n",
    "                        xx = str(xx)\n",
    "                        chunk1 = []\n",
    "                        output.append([row[0], xx])\n",
    "                        i = 1\n",
    "                if len(chunk1) > 0:\n",
    "                    xx = ''.join(chunk1)\n",
    "                    xx = str(xx)\n",
    "                    chunk1 = []\n",
    "                    output.append([row[0], xx])\n",
    "                    i = 1\n",
    "\n",
    "            df = pd.DataFrame(output, columns=[\"author_id\", \"doc_content\"])\n",
    "            print(df.dtypes)\n",
    "            print(\"Data Frame created: Shape: %s\" % (str(df.shape)))\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        print('Error %s' % e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    \n",
    "    print df\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution completed\n",
      "Read completed\n",
      "Number of rows: 61\n",
      "author_id       int64\n",
      "doc_content    object\n",
      "dtype: object\n",
      "Data Frame created: Shape: (21031, 2)\n",
      "       author_id                                        doc_content\n",
      "0             75  \\r\\n\\r\\n\\r\\n\\r\\nTranscribed from the Thomas Ne...\n",
      "1             75  n into the under-world of London with an attit...\n",
      "2             75   many as a dozen at a time, and\\r\\ndaily march...\n",
      "3             75  y\\r\\npolitical aggregations than by individual...\n",
      "4             75  re, and why they are living there, and\\r\\nwhat...\n",
      "5             75  il-clearers, living\\r\\nsign-posts to all the w...\n",
      "6             75  red, we would be in position to identify\\r\\nth...\n",
      "7             75  t is unprecedented, but--\"\\r\\n\\r\\n\"As I was ab...\n",
      "8             75  us personage who had\\r\\nimperturbably driven m...\n",
      "9             75   were filled with a new and different race of ...\n",
      "10            75  nd for the first\\r\\ntime in my life the fear o...\n",
      "11            75  afterwards he pulled up to the curb and inform...\n",
      "12            75  the water, was what he took my measure for--in...\n",
      "13            75  sy nothin' of the coat an' cap an' new stoker'...\n",
      "14            75   my shoes (not without\\r\\nregret for their lig...\n",
      "15            75  nding of the uppers with my\\r\\nfists that I wa...\n",
      "16            75  tion I had hitherto received, I now\\r\\nshared ...\n",
      "17            75  ped the pestilence of tipping, and\\r\\nencounte...\n",
      "18            75  hey talked as one man to another, and they tal...\n",
      "19            75  , while it has an air of desertion, so few are...\n",
      "20            75  that was all there was to it.  But I lingered,...\n",
      "21            75  doorstep and waited.\\r\\n\\r\\nAnd here to the do...\n",
      "22            75  h was on a level with a\\r\\nsidewalk, and in th...\n",
      "23            75  a name given him by a convicted felon in the d...\n",
      "24            75  nformation\\r\\nI might loudly vouchsafe.  And t...\n",
      "25            75  \\r\\n\\r\\nThis they indignantly denied, with sna...\n",
      "26            75  dely furnished,\\r\\nuncomfortable, and small.  ...\n",
      "27            75  ore my wife and babies and\\r\\nchattels.  There...\n",
      "28            75  \\r\\n\\r\\nNot only did the houses I investigated...\n",
      "29            75  estion.  \"This street is the\\r\\nvery last.  Al...\n",
      "...          ...                                                ...\n",
      "21001         75  xt that it was\\r\\nall your fault.  [LORETTA no...\n",
      "21002         75  ?\\r\\n\\r\\nNED.  [Calmly.]  Yes, there will be s...\n",
      "21003         75   what I want.  I wish I\\r\\nwere dead.\\r\\n\\r\\nN...\n",
      "21004         75  hat else did he say?\\r\\n\\r\\nLORETTA.  He said ...\n",
      "21005         75  wo scandals.\\r\\n\\r\\nNED.  To kiss the woman I ...\n",
      "21006         75  Almost shouting.]  You can't marry both of us!...\n",
      "21007         75  \\n[JACK HEMINGWAY chuckles.]\\r\\n\\r\\n[NED and L...\n",
      "21008         75  elated and at the same time frightened.  Her e...\n",
      "21009         75  and looking up at portrait\\r\\nmurmurs.]  Oh, y...\n",
      "21010         75  nd bonnet on floor.  He lays\\r\\ncigarette case...\n",
      "21011         75  queer one, reading a paper upside\\r\\ndown.\\r\\n...\n",
      "21012         75   hand\\r\\nand pumping her arm.]\\r\\n\\r\\nMAUD.  [...\n",
      "21013         75   you I saved you from a good licking right the...\n",
      "21014         75  arm, glancing at punching bag.]  No, I've got ...\n",
      "21015         75  s a perfect falsetto.\\r\\n\\r\\n[FITZSIMMONS nods...\n",
      "21016         75   any birth mark at all.\\r\\n\\r\\nMAUD.  I have, ...\n",
      "21017         75  on't.  They've all gone to\\r\\nthe fight.  Ther...\n",
      "21018         75  ong FITZSIMMONS turns his head slowly and look...\n",
      "21019         75  ng of the sort.\\r\\nI've--I've reformed.\\r\\n\\r\\...\n",
      "21020         75  .]  You know her, then?\\r\\n\\r\\nFITZSIMMONS.  Y...\n",
      "21021         75   school.\\r\\n\\r\\nMAUD.  [Springing to her feet ...\n",
      "21022         75  \\n\\r\\nFITZSIMMONS.  There are crooked men in e...\n",
      "21023         75  uck.  Younger fighters were coming up, and he ...\n",
      "21024         75  \\nAnd there he was, matched to fight with me, ...\n",
      "21025         75  s fighting for\\r\\nmore than I was fighting for...\n",
      "21026         75  ssus sitting up and waiting for Bill to come h...\n",
      "21027         75  you how glad I am you told me that.\\r\\n\\r\\nFIT...\n",
      "21028         75  ve you a reason--a--a good one.\\r\\nI--I--am no...\n",
      "21029         75  et me see you safely out of here.\\r\\n\\r\\nMAUD....\n",
      "21030         75  on.\\r\\n\\r\\nMAUD.  But you weren't a bit bashfu...\n",
      "\n",
      "[21031 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "authors=[123, 80, 75]\n",
    "doc = 204\n",
    "df = getWordAuthData(authors, doc, documentTable = 'aman_content', chunk_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 75  80 123]\n"
     ]
    }
   ],
   "source": [
    "print df.author_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    labels = []\n",
    "    texts = []\n",
    "    size = []\n",
    "    authorList = textToUse.author_id.unique()\n",
    "    for auth in authorList:\n",
    "        current = textToUse.loc[textToUse['author_id'] == auth]\n",
    "        size.append(current.shape[0])\n",
    "        print(\"Author: %5s  Size: %5s\" % (auth, current.shape[0]))\n",
    "    print(\"Min: %s\" % (min(size)))\n",
    "    print(\"Max: %s\" % (max(size)))\n",
    "\n",
    "    authorList = authorList.tolist()\n",
    "\n",
    "    for auth in authorList:\n",
    "        current = textToUse.loc[textToUse['author_id'] == auth]\n",
    "        if (samples > min(size)):\n",
    "            samples = min(size)\n",
    "        current = current.sample(n = samples)\n",
    "        textlist = current.doc_content.tolist()\n",
    "        texts = texts + textlist\n",
    "        labels = labels + [authorList.index(author_id) for author_id in current.author_id.tolist()]\n",
    "    labels_index = {}\n",
    "    labels_index[0] = 0\n",
    "    for i, auth in enumerate(authorList):\n",
    "        labels_index[i] = auth\n",
    "\n",
    "    del textToUse\n",
    "\n",
    "    print('Authors %s.' % (str(authorList)))\n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels))\n",
    "\n",
    "return (texts, labels, labels_index, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multilingual\n",
    "\n",
    "result = multilingual.main(authorlist, doc_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
