{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is called from Main and expects train and test values for x and y\n",
    "def load_ag_data(authors = None, docID = None): \n",
    "    \n",
    "    import data\n",
    "    train = data.getCharAuthorData(authors, docID) #Pass it to data and it returns a data frame\n",
    "    train = train.dropna()\n",
    "\n",
    "    labels = [] #\n",
    "    texts = []\n",
    "    size = []\n",
    "    authorList = train.author_id.unique()\n",
    "    for auth in authorList:\n",
    "        current = train.loc[train['author_id'] == auth]\n",
    "        size.append(current.shape[0])\n",
    "        print(\"Author: %5s  Size: %5s\" % (auth, current.shape[0]))\n",
    "    print(\"Min: %s\" % (min(size)))\n",
    "    print(\"Max: %s\" % (max(size)))\n",
    "\n",
    "    authorList = authorList.tolist()\n",
    "\n",
    "    for auth in authorList:\n",
    "        current = train.loc[train['author_id'] == auth]\n",
    "        samples = min(size)\n",
    "        current = current.sample(n = samples)\n",
    "        textlist = current.doc_content.tolist()\n",
    "        texts = texts + textlist\n",
    "        labels = labels + [authorList.index(author_id) for author_id in current.author_id.tolist()]\n",
    "    labels_index = {}\n",
    "    labels_index[0] = 0\n",
    "    for i, auth in enumerate(authorList):\n",
    "        labels_index[i] = auth\n",
    "\n",
    "    del train\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    labels = to_categorical(labels)\n",
    "    \n",
    "    print('Authors %s.' % (str(authorList)))\n",
    "    print('Found %s texts.' % len(texts))\n",
    "    print('Found %s labels.' % len(labels))\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    trainX, valX, trainY, valY = train_test_split(texts, labels, test_size= 0.2)\n",
    "    \n",
    "    \n",
    "    # return (texts, labels, labels_index, samples)\n",
    "\n",
    "\n",
    "    return ((trainX, trainY), (valX, valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_data(x, maxlen, vocab, vocab_size, check):\n",
    "    #Iterate over the loaded data and create a matrix of size maxlen x vocabsize\n",
    "    #In this case that will be 1014x69. This is then placed in a 3D matrix of size\n",
    "    #data_samples x maxlen x vocab_size. Each character is encoded into a one-hot\n",
    "    #array. Chars not in the vocab are encoded into an all zero vector.\n",
    "\n",
    "    input_data = np.zeros((len(x), maxlen, vocab_size))\n",
    "    for dix, sent in enumerate(x):\n",
    "        counter = 0\n",
    "        sent_array = np.zeros((maxlen, vocab_size))\n",
    "        chars = list(sent.replace(' ', ''))\n",
    "        for c in chars:\n",
    "            if counter >= maxlen:\n",
    "                pass\n",
    "            else:\n",
    "                char_array = np.zeros(vocab_size, dtype=np.int)\n",
    "                if c in check:\n",
    "                    ix = vocab[c]\n",
    "                    char_array[ix] = 1\n",
    "                sent_array[counter, :] = char_array\n",
    "                counter += 1\n",
    "        input_data[dix, :, :] = sent_array\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def create_vocab_set():\n",
    "    #This is a Unicode Character set\n",
    "    import string\n",
    "    unicode_characters = [];\n",
    "    for k in range(0,255) :\n",
    "        unicode_characters.append(unichr(k))\n",
    "    for k in range(1024, 1280) :\n",
    "        unicode_characters.append(unichr(k))\n",
    "        \n",
    "    #alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n",
    "    #            list(string.punctuation) + ['\\n'])\n",
    "    \n",
    "    alphabet = unicode_characters\n",
    "    vocab_size = len(alphabet)\n",
    "    check = set(alphabet)\n",
    "    vocab = {}\n",
    "    reverse_vocab = {}\n",
    "    for ix, t in enumerate(alphabet):\n",
    "        vocab[t] = ix\n",
    "        reverse_vocab[ix] = t\n",
    "\n",
    "\n",
    "    return vocab, reverse_vocab, vocab_size, check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (vocab, reverse_vocab, vocab_size, check) = create_vocab_set()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
